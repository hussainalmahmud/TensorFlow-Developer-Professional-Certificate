{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"C2_W2_Assignment_2.ipynb","provenance":[{"file_id":"https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/25_august_2021_fixes/C2/W2/assignment/C2_W2_Assignment.ipynb","timestamp":1643166657486}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"code","metadata":{"id":"zX4Kg8DUTKWO"},"source":["#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"smV6hLJqv54G"},"source":["**IMPORTANT NOTE:** This notebook is designed to run as a Colab. Click the button on top that says, `Open in Colab`, to run this notebook as a Colab. Running the notebook on your local machine might result in some of the code blocks throwing errors."]},{"cell_type":"code","metadata":{"id":"dn-6c02VmqiN"},"source":["# In this exercise you will train a CNN on the FULL Cats-v-dogs dataset\n","# This will require you doing a lot of data preprocessing because\n","# the dataset isn't split into training and validation for you\n","# This code block has all the required inputs\n","import os\n","import zipfile\n","import random\n","import tensorflow as tf\n","from tensorflow.keras.optimizers import RMSprop\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from shutil import copyfile"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3sd9dQWa23aj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643173256595,"user_tz":-540,"elapsed":25834,"user":{"displayName":"Hussain San","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjJg9CSDucmxYK-Ouw1GqIPXwR4zlXVC5BK3nRZ=s64","userId":"00602600267968326840"}},"outputId":"3808ae88-0c1e-4a1e-c434-c894f8bce1f0"},"source":["# This code block downloads the full Cats-v-Dogs dataset and stores it as \n","# cats-and-dogs.zip. It then unzips it to /tmp\n","# which will create a tmp/PetImages directory containing subdirectories\n","# called 'Cat' and 'Dog' (that's how the original researchers structured it)\n","# If the URL doesn't work, \n","# .   visit https://www.microsoft.com/en-us/download/confirmation.aspx?id=54765\n","# And right click on the 'Download Manually' link to get a new URL\n","\n","!wget --no-check-certificate \\\n","    \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\" \\\n","    -O \"/tmp/cats-and-dogs.zip\"\n","\n","local_zip = '/tmp/cats-and-dogs.zip'\n","zip_ref = zipfile.ZipFile(local_zip, 'r')\n","zip_ref.extractall('/tmp')\n","zip_ref.close()\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-01-26 05:00:31--  https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\n","Resolving download.microsoft.com (download.microsoft.com)... 23.11.208.107, 2600:1407:f800:4a5::e59, 2600:1407:f800:49b::e59\n","Connecting to download.microsoft.com (download.microsoft.com)|23.11.208.107|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 824894548 (787M) [application/octet-stream]\n","Saving to: ‘/tmp/cats-and-dogs.zip’\n","\n","/tmp/cats-and-dogs. 100%[===================>] 786.68M   127MB/s    in 6.8s    \n","\n","2022-01-26 05:00:39 (116 MB/s) - ‘/tmp/cats-and-dogs.zip’ saved [824894548/824894548]\n","\n"]}]},{"cell_type":"code","metadata":{"id":"gi3yD62a6X3S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643167013097,"user_tz":-540,"elapsed":224,"user":{"displayName":"Hussain San","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjJg9CSDucmxYK-Ouw1GqIPXwR4zlXVC5BK3nRZ=s64","userId":"00602600267968326840"}},"outputId":"2134e322-7d64-4aed-8d84-d02d63ce3355"},"source":["print(len(os.listdir('/tmp/PetImages/Cat/')))\n","print(len(os.listdir('/tmp/PetImages/Dog/')))\n","# Expected Output:\n","# 12501\n","# 12501"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["12501\n","12501\n"]}]},{"cell_type":"code","metadata":{"id":"F-QkLjxpmyK2"},"source":["# Use os.mkdir to create your directories\n","# You will need a directory for cats-v-dogs, and subdirectories for training\n","# and testing. These in turn will need subdirectories for 'cats' and 'dogs'\n","try:\n","  ### START CODE HERE\n","  os.mkdir('/tmp/cats-v-dogs/')\n","  os.mkdir('/tmp/cats-v-dogs/training')\n","  os.mkdir('/tmp/cats-v-dogs/testing')\n","  os.mkdir('/tmp/cats-v-dogs/training/cats')\n","  os.mkdir('/tmp/cats-v-dogs/training/dogs')\n","  os.mkdir('/tmp/cats-v-dogs/testing/cats')\n","  os.mkdir('/tmp/cats-v-dogs/testing/dogs')\n","  ### END CODE HERE\n","except OSError:\n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zvSODo0f9LaU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643174454986,"user_tz":-540,"elapsed":7085,"user":{"displayName":"Hussain San","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjJg9CSDucmxYK-Ouw1GqIPXwR4zlXVC5BK3nRZ=s64","userId":"00602600267968326840"}},"outputId":"b0d8e657-90fb-47f4-91f1-6f76188590a2"},"source":["# Write a python function called split_data which takes\n","# a SOURCE directory containing the files\n","# a TRAINING directory that a portion of the files will be copied to\n","# a TESTING directory that a portion of the files will be copie to\n","# a SPLIT SIZE to determine the portion\n","# The files should also be randomized, so that the training set is a random\n","# X% of the files, and the test set is the remaining files\n","# SO, for example, if SOURCE is PetImages/Cat, and SPLIT SIZE is .9\n","# Then 90% of the images in PetImages/Cat will be copied to the TRAINING dir\n","# and 10% of the images will be copied to the TESTING dir\n","# Also -- All images should be checked, and if they have a zero file length,\n","# they will not be copied over\n","#\n","# os.listdir(DIRECTORY) gives you a listing of the contents of that directory\n","# os.path.getsize(PATH) gives you the size of the file\n","# copyfile(source, destination) copies a file from source to destination\n","# random.sample(list, len(list)) shuffles a list\n","def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n","  #SOURCE: is the dir that contains original data\n","  #Training: to copy training files to \n","  #Testing: to copy test files to\n","\n","  ### START CODE HERE\n","  #First Check if the images don't have a zero vallue\n","  files = []\n","  for filename in os.listdir(SOURCE):\n","      file = SOURCE + filename\n","      if os.path.getsize(file) > 0:\n","          files.append(filename)\n","      else:\n","          print(filename + \" is zero length, so ignoring.\")\n","  #SPlit the data\n","  train_num = int(len(files) * SPLIT_SIZE)\n","  test_num = int(len(files) - train_num)\n","  randomized = random.sample(files, len(files))\n","  train_set = randomized[0:train_num]\n","  test_set = randomized[:test_num]\n","\n","  #Copy data to each file\n","  for filename in train_set:\n","    the_file = SOURCE + filename # FILES FROM SOURCE\n","    dest = TRAINING + filename # THE DESTINATION\n","    copyfile(the_file,dest)\n","  for filename in test_set:\n","    the_file = SOURCE + filename \n","    destination = TESTING + filename\n","    copyfile(the_file, destination)\n","  ### END CODE HERE\n","\n","\n","CAT_SOURCE_DIR = \"/tmp/PetImages/Cat/\"\n","TRAINING_CATS_DIR = \"/tmp/cats-v-dogs/training/cats/\"\n","TESTING_CATS_DIR = \"/tmp/cats-v-dogs/testing/cats/\"\n","DOG_SOURCE_DIR = \"/tmp/PetImages/Dog/\"\n","TRAINING_DOGS_DIR = \"/tmp/cats-v-dogs/training/dogs/\"\n","TESTING_DOGS_DIR = \"/tmp/cats-v-dogs/testing/dogs/\"\n","\n","split_size = .9\n","split_data(CAT_SOURCE_DIR, TRAINING_CATS_DIR, TESTING_CATS_DIR, split_size)\n","split_data(DOG_SOURCE_DIR, TRAINING_DOGS_DIR, TESTING_DOGS_DIR, split_size)\n","\n","# Expected output\n","# 666.jpg is zero length, so ignoring\n","# 11702.jpg is zero length, so ignoring"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["666.jpg is zero length, so ignoring.\n","11702.jpg is zero length, so ignoring.\n"]}]},{"cell_type":"code","metadata":{"id":"luthalB76ufC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643174454993,"user_tz":-540,"elapsed":20,"user":{"displayName":"Hussain San","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjJg9CSDucmxYK-Ouw1GqIPXwR4zlXVC5BK3nRZ=s64","userId":"00602600267968326840"}},"outputId":"3b83c305-b459-47f7-ae1e-47c5b94bc414"},"source":["print(len(os.listdir('/tmp/cats-v-dogs/training/cats/')))\n","print(len(os.listdir('/tmp/cats-v-dogs/training/dogs/')))\n","print(len(os.listdir('/tmp/cats-v-dogs/testing/cats/')))\n","print(len(os.listdir('/tmp/cats-v-dogs/testing/dogs/')))\n","# Expected output:\n","# 11250\n","# 11250\n","# 1250\n","# 1250"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["12498\n","12500\n","2380\n","2385\n"]}]},{"cell_type":"code","metadata":{"id":"-BQrav4anTmj"},"source":["# DEFINE A KERAS MODEL TO CLASSIFY CATS V DOGS\n","# USE AT LEAST 3 CONVOLUTION LAYERS\n","model = tf.keras.models.Sequential([\n","    ### START CODE HERE                                \n","    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150,150,3)),\n","    tf.keras.layers.MaxPool2D((2,2)),\n","    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n","    tf.keras.layers.MaxPool2D((2,2)),\n","    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n","    tf.keras.layers.MaxPool2D((2,2)),\n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.Dense(512, activation='relu'),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","\n","    ### END CODE HERE\n","])\n","\n","model.compile(optimizer=RMSprop(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mlNjoJ5D61N6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643179184689,"user_tz":-540,"elapsed":888,"user":{"displayName":"Hussain San","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjJg9CSDucmxYK-Ouw1GqIPXwR4zlXVC5BK3nRZ=s64","userId":"00602600267968326840"}},"outputId":"48124353-4d6c-4d80-d47b-2355298edd44"},"source":["TRAINING_DIR = '/tmp/cats-v-dogs/training/'#YOUR CODE HERE\n","train_datagen = ImageDataGenerator(\n","    rescale= 1/255.0,\n","    rotation_range=40,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest'\n",")#YOUR CODE HERE\n","train_generator = train_datagen.flow_from_directory(\n","    TRAINING_DIR,\n","    batch_size=100,\n","    class_mode='binary',\n","    target_size=(150,150)\n",")#YOUR CODE HERE\n","\n","VALIDATION_DIR = '/tmp/cats-v-dogs/testing/' #YOUR CODE HERE\n","validation_datagen = ImageDataGenerator(\n","    rescale= 1/255.0,\n","    rotation_range=40,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest'\n",")#YOUR CODE HERE\n","validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR,batch_size=100, class_mode='binary',target_size=(150,150)) #YOUR CODE HERE\n","\n","\n","\n","# Expected Output:\n","# Found 22498 images belonging to 2 classes.\n","# Found 2500 images belonging to 2 classes."],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 24996 images belonging to 2 classes.\n","Found 4765 images belonging to 2 classes.\n"]}]},{"cell_type":"markdown","metadata":{"id":"clJjYTDpDnIA"},"source":["Note: You can ignore the `UserWarning: Possibly corrupt EXIF data.` warnings."]},{"cell_type":"code","metadata":{"id":"KyS4n53w7DxC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643182399821,"user_tz":-540,"elapsed":3211640,"user":{"displayName":"Hussain San","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjJg9CSDucmxYK-Ouw1GqIPXwR4zlXVC5BK3nRZ=s64","userId":"00602600267968326840"}},"outputId":"232cc759-fdac-4a6b-cde3-1938aa36c033"},"source":["# Note that this may take some time.\n","history = model.fit(train_generator,\n","                              epochs=15,\n","                              verbose=1,\n","                              validation_data=validation_generator,\n","                              validation_steps=8\n","                    )\n","\n","# The expectation here is that the model will train, and that accuracy will be > 95% on both training and validation\n","# i.e. acc:A1 and val_acc:A2 will be visible, and both A1 and A2 will be > .9"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/15\n","164/250 [==================>...........] - ETA: 1:12 - loss: 0.7745 - accuracy: 0.5787"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 32 bytes but only got 0. Skipping tag 270\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 5 bytes but only got 0. Skipping tag 271\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 272\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 282\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 283\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 20 bytes but only got 0. Skipping tag 306\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 48 bytes but only got 0. Skipping tag 532\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n","  warnings.warn(str(msg))\n"]},{"output_type":"stream","name":"stdout","text":["250/250 [==============================] - 218s 866ms/step - loss: 0.7257 - accuracy: 0.5990 - val_loss: 0.5863 - val_accuracy: 0.6900\n","Epoch 2/15\n","250/250 [==============================] - 215s 859ms/step - loss: 0.5969 - accuracy: 0.6777 - val_loss: 0.5661 - val_accuracy: 0.7113\n","Epoch 3/15\n","250/250 [==============================] - 214s 857ms/step - loss: 0.5604 - accuracy: 0.7100 - val_loss: 0.5161 - val_accuracy: 0.7525\n","Epoch 4/15\n","250/250 [==============================] - 217s 867ms/step - loss: 0.5376 - accuracy: 0.7306 - val_loss: 0.5066 - val_accuracy: 0.7538\n","Epoch 5/15\n","250/250 [==============================] - 216s 865ms/step - loss: 0.5207 - accuracy: 0.7415 - val_loss: 0.4875 - val_accuracy: 0.7688\n","Epoch 6/15\n","250/250 [==============================] - 214s 856ms/step - loss: 0.5034 - accuracy: 0.7570 - val_loss: 0.4336 - val_accuracy: 0.7912\n","Epoch 7/15\n","250/250 [==============================] - 212s 848ms/step - loss: 0.4894 - accuracy: 0.7661 - val_loss: 0.4592 - val_accuracy: 0.7962\n","Epoch 8/15\n","250/250 [==============================] - 213s 852ms/step - loss: 0.4795 - accuracy: 0.7703 - val_loss: 0.4382 - val_accuracy: 0.8050\n","Epoch 9/15\n","250/250 [==============================] - 213s 850ms/step - loss: 0.4676 - accuracy: 0.7823 - val_loss: 0.4405 - val_accuracy: 0.7987\n","Epoch 10/15\n","250/250 [==============================] - 212s 848ms/step - loss: 0.4551 - accuracy: 0.7861 - val_loss: 0.4060 - val_accuracy: 0.8100\n","Epoch 11/15\n","250/250 [==============================] - 214s 856ms/step - loss: 0.4496 - accuracy: 0.7897 - val_loss: 0.4080 - val_accuracy: 0.8250\n","Epoch 12/15\n","250/250 [==============================] - 212s 848ms/step - loss: 0.4349 - accuracy: 0.7988 - val_loss: 0.4214 - val_accuracy: 0.8100\n","Epoch 13/15\n","250/250 [==============================] - 213s 854ms/step - loss: 0.4362 - accuracy: 0.8001 - val_loss: 0.4264 - val_accuracy: 0.8213\n","Epoch 14/15\n","250/250 [==============================] - 214s 854ms/step - loss: 0.4244 - accuracy: 0.8064 - val_loss: 0.5662 - val_accuracy: 0.7175\n","Epoch 15/15\n","250/250 [==============================] - 213s 853ms/step - loss: 0.4196 - accuracy: 0.8097 - val_loss: 0.4693 - val_accuracy: 0.7738\n"]}]},{"cell_type":"code","metadata":{"id":"MWZrJN4-65RC"},"source":["# PLOT LOSS AND ACCURACY\n","%matplotlib inline\n","\n","import matplotlib.image  as mpimg\n","import matplotlib.pyplot as plt\n","\n","#-----------------------------------------------------------\n","# Retrieve a list of list results on training and test data\n","# sets for each training epoch\n","#-----------------------------------------------------------\n","acc=history.history['accuracy']\n","val_acc=history.history['val_accuracy']\n","loss=history.history['loss']\n","val_loss=history.history['val_loss']\n","\n","epochs=range(len(acc)) # Get number of epochs\n","\n","#------------------------------------------------\n","# Plot training and validation accuracy per epoch\n","#------------------------------------------------\n","plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n","plt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n","plt.title('Training and validation accuracy')\n","plt.figure()\n","\n","#------------------------------------------------\n","# Plot training and validation loss per epoch\n","#------------------------------------------------\n","plt.plot(epochs, loss, 'r', \"Training Loss\")\n","plt.plot(epochs, val_loss, 'b', \"Validation Loss\")\n","\n","\n","plt.title('Training and validation loss')\n","\n","# Desired output. Charts with training and validation metrics. No crash :)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MH2xBmfhv_FG"},"source":["**Important Note:** Due to some compatibility issues, the following code block will result in an error after you select the images(s) to upload if you are running this notebook as a `Colab` on the `Safari` browser. For `all other broswers`, continue with the next code block and ignore the next one after it.\n","\n","The ones running the `Colab` on `Safari`, comment out the code block below, uncomment the next code block and run it."]},{"cell_type":"code","metadata":{"id":"LqL6FYUrtXpf"},"source":["# Here's a codeblock just for fun. You should be able to upload an image here \n","# and have it classified without crashing\n","\n","import numpy as np\n","from google.colab import files\n","from keras.preprocessing import image\n","\n","uploaded = files.upload()\n","\n","for fn in uploaded.keys():\n"," \n","  # predicting images\n","  path = '/content/' + fn\n","  img = image.load_img(path, target_size=(# YOUR CODE HERE))\n","  x = image.img_to_array(img)\n","  x = np.expand_dims(x, axis=0)\n","\n","  images = np.vstack([x])\n","  classes = model.predict(images, batch_size=10)\n","  print(classes[0])\n","  if classes[0]>0.5:\n","    print(fn + \" is a dog\")\n","  else:\n","    print(fn + \" is a cat\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3TtdrEH6v_v6"},"source":["For those running this `Colab` on `Safari` broswer can upload the images(s) manually. Follow the instructions, uncomment the code block below and run it.\n","\n","Instructions on how to upload image(s) manually in a Colab:\n","\n","1. Select the `folder` icon on the left `menu bar`.\n","2. Click on the `folder with an arrow pointing upwards` named `..`\n","3. Click on the `folder` named `tmp`.\n","4. Inside of the `tmp` folder, `create a new folder` called `images`. You'll see the `New folder` option by clicking the `3 vertical dots` menu button next to the `tmp` folder.\n","5. Inside of the new `images` folder, upload an image(s) of your choice, preferably of either a horse or a human. Drag and drop the images(s) on top of the `images` folder.\n","6. Uncomment and run the code block below. "]},{"cell_type":"code","metadata":{"id":"yV4bEFcqwALj"},"source":["# import numpy as np\n","# from keras.preprocessing import image\n","# import os\n","\n","# images = os.listdir(\"/tmp/images\")\n","\n","# print(images)\n","\n","# for i in images:\n","#  print()\n","#  # predicting images\n","#  path = '/tmp/images/' + i\n","#  img = image.load_img(path, target_size=(150, 150))\n","#  x = image.img_to_array(img)\n","#  x = np.expand_dims(x, axis=0)\n","\n","#  images = np.vstack([x])\n","#  classes = model.predict(images, batch_size=10)\n","#  print(classes[0])\n","#  if classes[0]>0.5:\n","#    print(i + \" is a dog\")\n","#  else:\n","#    print(i + \" is a cat\")"],"execution_count":null,"outputs":[]}]}